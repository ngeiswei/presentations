\documentclass[aspectratio=169]{beamer}

\usepackage{beamerthemesplit}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{bussproofs}
%% \usepackage{tkz-graph}

\makeatletter
\newcommand{\reallytiny}{\@setfontsize{\srcsize}{2pt}{2pt}}
\makeatother

\mode<presentation>
{
  \usetheme{AnnArbor}
  \usecolortheme{crane}
}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}

\title{AGI-20 Tutorial\\ OpenCog, PLN and Pattern Miner}

\author{Nil Geisweiller, Matt Ikle}

\institute[SingularityNET OpenCog Foundations]
{
  \begin{center}
    SingularityNET \& OpenCog Foundations\\
    \includegraphics[scale=0.32]{images/snet_oc.png}
  \end{center}
}
          
\date[AGI-20]

\begin{document}

\begin{frame}
  %% It's gonna be a tutorial mainly focused on reasoning using
  %% OpenCog, although as you'll see it's actually quite broad as that
  %% includes some forms of learning, such as pattern mining, which
  %% we'll go over as well.

  \maketitle
\end{frame}

\section {Preparation}

\begin{frame}
  %% Because there's a docker image to download for this tutorial,
  %% we're going to take care of that first, and while it's
  %% downloading in the background I'll give a short introduction to
  %% OpenCog and what we're gonna cover.

  %% \frametitle{Preparation}
  \begin{enumerate}
  \item Install docker
    \begin{itemize}
    \item Debian/Ubuntu
      \begin{semiverbatim}sudo apt install docker.io\end{semiverbatim}
      \item Arch/Manjaro
        \begin{semiverbatim}sudo pacman -S docker\end{semiverbatim}
    \end{itemize}
  \item Download docker image (1.6GB)
    \begin{semiverbatim}sudo docker pull ngeiswei/opencog:agi20\end{semiverbatim}      
  \end{enumerate}
\end{frame}

\section {OpenCog}

\begin{frame}
  %% OpenCog is a framework for doing AGI research and solve real
  %% problems as well.  It's been used for many things such as
  %% controlling virtual or real agents, for bio-informatics, the list
  %% is long.

  %% What it offers are

  %% 1. A hypergraph database, call the AtomSpace with a language,
  %%    called Atomese, to query, rewrite and generally operate over
  %%    the atomspace.

  %% 2. Cognitive processes, called Mind Agents, for reasoning (that's
  %%    what we'll focus on here), learning, making decision, natural
  %%    language processing, and more things, such as what Alexey
  %%    Potapov and his team are working on, and that they will have a
  %%    chance to present during the conference.

  %% \frametitle{OpenCog}

  \center{Framework for AGI}\\[0.5cm]

  \begin{columns}
    \column{3in}
  
    \begin{enumerate}
    \item Hypergraph Database: 
      \begin{itemize}
      \item AtomSpace
      \item Atomese: query, rewrite and more
      \end{itemize}
    \item Mind Agents (cognitive processes):
      \begin{itemize}
      \item \alert{Reasoning: PLN, Miner}
      \item Learning: MOSES, Miner
      \item Decision: OpenPsi (Bach's MicroPsi)
      \item Language Processing
      \item Attention Allocation
      \end{itemize}
    \end{enumerate}

    \column{3in}

    \includegraphics[scale=0.2]{images/ng2-atomspace-visualizer.jpg}

  \end{columns}

  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\[0.1cm]
  \center{Download docker image: \texttt{\textcolor{black}{sudo docker pull ngeiswei/opencog:agi20}}}

\end{frame}

\section {PLN}

\begin{frame}
  %% I'm gonna give a quit presentation of what is PLN, which should
  %% help you to understand what is going on during the tutorial.

  %% PLN stands for Probabilistic Logic Network, it was invented by
  %% Ben, Matt and a few others. It was really created specifically
  %% for the AtomSpace, to handle common sense reasoning, and just
  %% about every type of reasoning that's useful for dealing with the
  %% king declarative knowledge that is stored in the AtomSpace (you
  %% may correct me Matt if I'm saying something inaccurate).

  %% It basically builds upon Probability Theory it to handle
  %% uncertainty (with a probabilistic semantics, I'll show that
  %% next), but it's also a logical system, more or less a super-set
  %% of predicate logic.  And although it's particularly well suited
  %% for common sense reasoning it can handle mathematical reasoning
  %% as well, like a theorem prover would, even though it's been
  %% rather under-explored so far.

  %% An important point about PLN and the way it's been used is that
  %% it can reason about its own resource management, therefore
  %% enabling a form of meta-reasoning, to improve its efficiency over
  %% time.

  %% \frametitle{PLN}

  \begin{columns}
    \column{2.5in}
    \includegraphics[scale=0.32]{images/PLN.jpg}\\

    \column{3in}
    \begin{itemize}
    \item Probability Theory
    \item Uncertainty management
    \item Common sense reasoning
    \item Mathematical reasoning
    \item Resource management
    \end{itemize}

  \end{columns}

  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\[0.1cm]
  \center{Download docker image: \texttt{\textcolor{black}{sudo docker pull ngeiswei/opencog:agi20}}}

\end{frame}

\subsection {Basics}

\begin{frame}
  %% TODO
  
  %% Or rather an estimate of that probability.  And c is the
  %% confidence, defined with that formula.  The formula itself
  %% doesn't really matter, what matters is that we have a monotonic
  %% mapping between the size of A and the range zero one, so that as
  %% the size goes to infinity the confidence goes to one.

  %% OK, but why is that called simple truth value? Well, as you
  %% probably, it's because we also have more complex truth values,
  %% the reason is because underneath a truth value is a second order
  %% distribution.

  %% \frametitle{PLN: the basics}

  \begin{columns}
    \column{1in}
    \includegraphics[scale=0.8]{images/subset_A_B.pdf}

    \column{2.7in}
\texttt{(Subset (stv s c)\\
\ \ \ \ A\\
\ \ \ \ B)}\\[0.22cm]

\begin{block}
  {Definitions:}
  {\begin{itemize}
    \item \texttt{stv} = \textit{Simple Truth Value}
    \item \texttt{Subset A B} = $P(B|A)$
    \item \texttt{s} = strength = $\displaystyle P(B|A)=\frac{|A \cap B|}{|A|}$
    \item \texttt{c} = confidence = $\displaystyle \frac{|A|}{|A| + K}$
    \end{itemize}}
\end{block}

  \end{columns}

  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\[0.1cm]
  \center{Download docker image: \texttt{\textcolor{black}{sudo docker pull ngeiswei/opencog:agi20}}}

\end{frame}

\begin{frame}
  %% So basically when the confidence in null the second order
  %% distribution is flat if you use a Bayesian prior, or slightly
  %% concave is you use a Jeffrey prior, and as measure as the number
  %% of observations grow the second order distribution narrows down
  %% to a particular value. OK, but that's if you model the second
  %% order distribution with a beta-distribution, which is justified a
  %% lot of the time, but not always, if you start doing inference and
  %% aggregate data from different sources, that sort of things, a
  %% beta-distribution might no longer adequate.  A beta-distribution
  %% has two parameters so has a Simple Truth Value, a strength and a
  %% confidence, so it's enough for that case, but in general it's
  %% not, which is why we need more complex types of truth value than
  %% simple truth value.

  %% \frametitle{PLN: the basics}

  \center{\textit{Truth Value = \textcolor{red}{Second Order Distribution}}}\\[1.45cm]

  \begin{columns}
    \column{1.65in}
    \includegraphics[scale=0.2]{images/jeffreys_prior.png}  

    \column{1.65in}
    \includegraphics[scale=0.2]{images/bayesian_prior.png}  

    \column{1.65in}
    \includegraphics[scale=0.2]{images/observations_10.png}  

    \column{1.65in}
    \includegraphics[scale=0.2]{images/observations_50.png}  
  \end{columns}

  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\[0.1cm]
  \center{Download docker image: \texttt{\textcolor{black}{sudo docker pull ngeiswei/opencog:agi20}}}

\end{frame}

\subsection{Example}

\begin{frame}

  {\small
    \begin{itemize}
    \item Knowledge base:\\
      \texttt{(Subset (stv 0.4 0.1) A B)}\\
      \texttt{(Subset (stv 0.6 0.2) B C)}\\
      \texttt{A (stv 0.1 0.6)}

    \item Rule base:\\
      \begin{prooftree}
        \AxiomC{\texttt{(Subset <tv1> X Y)}}
        \AxiomC{\texttt{(Subset <tv2> Y Z)}}
        \RightLabel{(Deduction)}
        \BinaryInfC{\texttt{(Subset <tv3> X Z)}}
      \end{prooftree}

      \begin{prooftree}
        \AxiomC{\texttt{X <tv1>}}
        \AxiomC{\texttt{(Subset <tv2> X Y)}}
        \RightLabel{(Modus Ponens)}
        \BinaryInfC{\texttt{Y <tv3>}}
      \end{prooftree}
        
    \item Target: \texttt{C <?>}

    \end{itemize}
  }

  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\[0.1cm]
  \center{Download docker image: \texttt{\textcolor{black}{sudo docker pull ngeiswei/opencog:agi20}}}

\end{frame}

\begin{frame}

  % And as it builds the inference tree, by extension, it's also gonna
  % build the graph of function calls to calculate the truth value of
  % the conclusion

  \begin{columns}

    \column{1in}
    
    \column{1.35in}
    \begin{block}
      {Unified Rule Engine}
      {\center{Rule base\\
        Target\\[0.45cm]
        $\downarrow$\\[0.45cm]
        \textcolor{red}{Build inference tree}}
      }
    \end{block}

    \column{1in}
\end{columns}

  {\small
    \begin{prooftree}
      \AxiomC{\texttt{A <?>}}
      \AxiomC{\texttt{(Subset <?> A B)}}
      \AxiomC{\texttt{(Subset <?> A B)}}
      \RightLabel{(Deduction)}
      \BinaryInfC{\texttt{(Subset <?> A C)}}
      \RightLabel{(Modus Ponens)}
      \BinaryInfC{\texttt{C <?>}}
    \end{prooftree}}

  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\[0.1cm]
  \center{Download docker image: \texttt{\textcolor{black}{sudo docker pull ngeiswei/opencog:agi20}}}

\end{frame}

\subsection{Overall}

\begin{frame}

  % Then you have the classic conjunction, disjunction, negation
  % introduction rules. Modus ponens, etc. And that's it for the
  % basics.

  % There's more to it than that, PLN incorporates quantifiers,
  % intentional reasoning (which is reasoning based on the properties
  % rather than members), contextual reasoning, and then on top of all
  % that you can add temporal and spatial reasoning, etc, but, with the
  % exception of quantifiers which is more like an extension, all this
  % extra stuff is built on top of that very simple base that I've just
  % presented.

  \begin{columns}
    \column{4in}
    
    Basic:
    \begin{itemize}
    \item Deduction
    \item Modus Ponens
    \item Conjunction/Disjunction/Negation Introduction
    \item Universal Instantiation
    \item ...
    \end{itemize}
    
    \column{2in}
    
    Advanced:
    \begin{itemize}
    \item Intensional
    \item Contextual
    \item Temporal
    \item Spatial
    \item ...
    \end{itemize}
    
  \end{columns}
\end{frame}

\begin{frame}

\end{frame}

\begin{frame}
  I also mentioned that reasoning can also used for learning. In
  OpenCog we at least have one component that's already built like
  that, this is the pattern miner.  The pattern miner allows to
  discover rather simple frequent patterns hiding in an atomspace.  By
  simple I don't mean that they are small BTW, I just mean that their
  size is not gonna be a very good measure of their Kolmogorov
  complexity.

  So here's a simple example, if

  A->B, A->C, A->D
  |-
  A->X

  we have a pattern which is that A implies something.
\end{frame}

\begin{frame}
  So how to map that pattern miner as a form of reasoning, we just
  formalize the following property, which is that if a pattern is
  frequent enough over some dataset, then an abstract of that pattern
  is frequent enough. In practice, the implementation is more
  trickier, and we also need to account for the surprisingness of the
  pattern, but that's the idea, or at least one of the ideas.
\end{frame}

\end{document}
